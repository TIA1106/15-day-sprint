# ğŸ¤– LangChain Agentic AI â€“ Sprint Logs

Welcome to my LangChain + LLM exploration as part of a 15-day AI & Dev sprint.  
Here I build, debug, and tune AI agents using LangChain and HuggingFace models â€” **no paid APIs needed**.

---

## âœ… Agents Built

---

### ğŸ§  Day 1 â€“ Basic LLM Prompting

ğŸ“ File: `hello_llm.py`  
ğŸ”¹ Goal: Basic text generation using `distilgpt2`

| Feature | Details |
|--------|---------|
| ğŸ› ï¸ Model Used | `distilgpt2` |
| ğŸ“¦ Tools | HuggingFace Transformers, LangChain |
| âš ï¸ Limitations | No factual QA, mostly text continuation |
| âŒ Problem | GPT-2 returned random, hallucinated stories |

---

### ğŸ§  Day 2 â€“ Q&A Chatbot (Flan-T5)

ğŸ“ File: `hello_llm_chat.py`  
ğŸ”¹ Goal: Create a terminal chatbot that answers questions meaningfully

| Feature | Details |
|--------|---------|
| ğŸ§  Model Used | `google/flan-t5-base` |
| ğŸ“¦ Tools | LangChain, HuggingFace Transformers |
| âœ… Prompting | Instruction-tuned: `"Answer the following question clearly and helpfully"` |
| âœ… Output | Clean, logical, consistent Q&A |
| ğŸ” Fixes | Replaced `.run()` with `.invoke()` and tuned tokens |
| ğŸ’¬ Example: |
You: What is gravity?
ğŸ¤– AI: Gravity is a natural force that attracts objects toward one another.

---

## ğŸ“Œ Learnings So Far

- Prompt tuning dramatically changes LLM output
- GPT-2 isnâ€™t ideal for factual QA â€” better for storytelling
- Flan-T5 is instruction-following and gives cleaner Q&A
- LangChain `invoke()` is more future-compatible than `run()`

---

# ğŸ¤– Day 3 â€“ AI Agent: Tone-Controlled Chatbot

### âœ… Tech:
- LangChain + HuggingFacePipeline (GPT-2)
- PromptTemplate for sarcasm-based tone

### âœ… What it does:
- Takes user question
- Responds in sarcastic/rude tone using prompt control

### ğŸš¨ Observations:
- GPT2 repeated prompt â†’ replaced with DialoGPT
- Learned limitations of small LLMs
- Prompt formatting (INST-style) improves results

ğŸ§  Day 4 â€“ Study Plan Generator with Flan-T5
ğŸ“ File: ai_study_planner.py
ğŸ”¹ Goal: Use a local HuggingFace model (flan-t5-base) to generate a 15-day study plan for any learning goal without using paid APIs.
| Feature          | Details                                                                                                     |
| ---------------- | ----------------------------------------------------------------------------------------------------------- |
| ğŸ§  Model Used    | `google/flan-t5-base`                                                                                       |
| ğŸ”— Tools         | `transformers`, `LangChain`, `HuggingFacePipeline`                                                          |
| ğŸ¯ Prompt        | "You are a strict mentor AI. Write a numbered 15-day study plan..."                                         |
| âœ… Outcome        | User can enter any learning goal (e.g., "learn React in 15 days") and get a clear, actionable day-wise plan |
| ğŸ’¡ Prompt Format | Multi-line instructions with bullet formatting + rules                                                      |
| ğŸ§ª Fixes         | Replaced broken `.run()` with `.invoke()`, ensured correct output formatting                                |
| ğŸš« No APIs       | No OpenAI key needed â€” everything runs locally using HuggingFace                                            |

ğŸ“Œ Learnings on Day 4
ğŸ§  HuggingFace models like Flan-T5 are great for instruction-following

âš ï¸ Prompt formatting matters â€” multi-line prompts with strict instructions improved reliability

ğŸ” .invoke() is now the standard in LangChain â€” smoother execution

ğŸ’» This task taught how to design prompt-controlled utility agents for real-world productivity tools (like planners)
**Logged by:** Tia Sukhnanni  
ğŸ“… Sprint Days 1â€“2 | ğŸ’» JKLU BTech | ğŸ§  AI + Web + Dev  
